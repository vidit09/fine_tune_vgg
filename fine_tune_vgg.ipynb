{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tune VGG for your dataset\n",
    "We start with VGG-16 network pretrained on Imagenet Dataset for 1000 categories. Modify the last fully connected layer according to number of classes in custom dataset.\n",
    "\n",
    "**Keras with Tensorflow as backend is used here.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train the model provide train and validation files. These file should be of the form\n",
    "\n",
    "```train.txt``` <br>\n",
    "```path/to/image1 label1```<br>\n",
    "```path/to/image2 label2```<br>\n",
    "```path/to/image3 label3```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Starting with some cool imports\n",
    "from keras.applications.vgg16 import (\n",
    "    VGG16, preprocess_input, decode_predictions)\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import image\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "from keras import utils\n",
    "from keras.callbacks import ModelCheckpoint,TensorBoard\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the pretrained VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_vgg_model(num_classes):\n",
    "    \n",
    "    #take keras pretrained model and remove last 3 fully connected layers\n",
    "    vgg = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "    #add the custom fc for your num_classes\n",
    "    last_conv = vgg.output\n",
    "\n",
    "    x = Flatten()(last_conv)\n",
    "    x = Dense(2048, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(vgg.input, x)\n",
    "\n",
    "    #only fine tune these fully connected layers\n",
    "    for layer in model.layers[:19]:\n",
    "#         print(layer)\n",
    "        layer.trainable = False\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function to create train and validation batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This class takes list of (image_path label) and returns a batches of specified size.\n",
    "# get_data function changes the input image size to VGG format(224x224x3) and applies normalization. \n",
    "class CustomDataGen():\n",
    "\n",
    "    def __init__(self, dim_x, dim_y, dim_z, num_class, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "        self.dim_x = dim_x\n",
    "        self.dim_y = dim_y\n",
    "        self.dim_z = dim_z\n",
    "        self.num_class = num_class\n",
    "\n",
    "    def randomize_ind(self,data):\n",
    "        indexes = np.arange(len(data))\n",
    "        np.random.shuffle(indexes)\n",
    "        return indexes\n",
    "\n",
    "    def get_data(self,list):\n",
    "\n",
    "        X = np.empty((self.batch_size, self.dim_x, self.dim_y, self.dim_z))\n",
    "        y = np.empty((self.batch_size,self.num_class))\n",
    "\n",
    "        for id, data in enumerate(list):\n",
    "            im_path = data.split(' ')[0]\n",
    "            label = int(data.split(' ')[1])\n",
    "            img = image.load_img(im_path, target_size=(self.dim_x, self.dim_y))\n",
    "            \n",
    "            x = image.img_to_array(img)\n",
    "            x = np.expand_dims(x, axis=0)\n",
    "            x = preprocess_input(x)[0]\n",
    "            X[id,:,:,:] = x\n",
    "\n",
    "            y_ = utils.to_categorical(label, self.num_class)\n",
    "            y[id,...] = y_\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def generate_batch(self, data):\n",
    "\n",
    "        while 1:\n",
    "            indexes = self.randomize_ind(data)\n",
    "\n",
    "            num_batch = int(len(indexes)/self.batch_size)\n",
    "            for batch_id in range(num_batch):\n",
    "                temp_list = [data[k] for k in indexes[batch_id*self.batch_size:(batch_id+1)*self.batch_size]]\n",
    "                X,y = self.get_data(temp_list)\n",
    "                yield X,y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function to read txt file to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_img_list_from_file(img_dir,file_path):\n",
    "\n",
    "    data = []\n",
    "    with open(file_path) as f:\n",
    "        for line in f:\n",
    "            data.append(img_dir + line)\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.topology.InputLayer object at 0x11624b860>\n",
      "<keras.layers.convolutional.Conv2D object at 0x11ae107f0>\n",
      "<keras.layers.convolutional.Conv2D object at 0x11c39f2e8>\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x11ae10b38>\n",
      "<keras.layers.convolutional.Conv2D object at 0x116335da0>\n",
      "<keras.layers.convolutional.Conv2D object at 0x116335a20>\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x115b34f28>\n",
      "<keras.layers.convolutional.Conv2D object at 0x115b5ea20>\n",
      "<keras.layers.convolutional.Conv2D object at 0x115b4efd0>\n",
      "<keras.layers.convolutional.Conv2D object at 0x1160aaf60>\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x1160d0a20>\n",
      "<keras.layers.convolutional.Conv2D object at 0x116e9aa90>\n",
      "<keras.layers.convolutional.Conv2D object at 0x121468be0>\n",
      "<keras.layers.convolutional.Conv2D object at 0x116e91ac8>\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x116e7cf98>\n",
      "<keras.layers.convolutional.Conv2D object at 0x11fb79860>\n",
      "<keras.layers.convolutional.Conv2D object at 0x116f2eeb8>\n",
      "<keras.layers.convolutional.Conv2D object at 0x11fba86d8>\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x11fb85b38>\n",
      "Epoch 1/30\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Get the train and test data\n",
    "directory = '../images/'\n",
    "\n",
    "train_data = read_img_list_from_file(directory,'../splits/train0.txt')\n",
    "val_data = read_img_list_from_file(directory,'../splits/test0.txt')\n",
    "\n",
    "#Specify the params\n",
    "batch_size = 100\n",
    "num_epoch = 30\n",
    "num_classes = 25\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Get the modified VGG model\n",
    "model = get_vgg_model(num_classes)\n",
    "\n",
    "#Specify the loss and optimizer\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizers.SGD(lr=learning_rate, momentum=0.9),\n",
    "              metrics=['categorical_accuracy'])\n",
    "\n",
    "#Data generators\n",
    "training_generator = CustomDataGen(224, 224, 3, num_classes, batch_size).generate_batch(train_data)\n",
    "validation_generator = CustomDataGen(224, 224, 3, num_classes, len(val_data)).generate_batch(val_data)\n",
    "\n",
    "#Save the checkpoints whenever improvement in accuracy is seen\n",
    "file_path = \"weights-improvement-{epoch:02d}-{val_categorical_accuracy:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_categorical_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "#Tensorboard visualization callback. track the losses while running\n",
    "tensorboard = TensorBoard(log_dir='./logs', histogram_freq=0, write_graph=True, write_images=False)\n",
    "\n",
    "callbacks_list = [checkpoint, tensorboard]\n",
    "\n",
    "#Start training\n",
    "model.fit_generator(generator = training_generator,\n",
    "                    steps_per_epoch = len(train_data)//batch_size,\n",
    "                    epochs= num_epoch,\n",
    "                    validation_data = validation_generator,\n",
    "                    validation_steps = 1,\n",
    "                    callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_classes=25\n",
    "inference_model = get_vgg_model(num_classes)\n",
    "inference_model.load_weights('/Users/vidit/Thesis/freiburg_groceries_dataset/fine_tune_vgg/weights_base_version.hdf5')\n",
    "\n",
    "#preprocess the image\n",
    "test_image_path = '/Users/vidit/Thesis/freiburg_groceries_dataset/images/COFFEE/COFFEE0006.png'\n",
    "test_img = image.load_img(test_image_path, target_size=(224, 224))\n",
    "test_img = image.img_to_array(test_img)\n",
    "test_img = np.expand_dims(x, axis=0)\n",
    "test_img = preprocess_input(x)\n",
    "\n",
    "predictions = inference_model.predict(test_img)\n",
    "predicted_class = np.argmax(predictions)\n",
    "\n",
    "print('Predicted class:{}'.format(predicted_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:python35]",
   "language": "python",
   "name": "conda-env-python35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
